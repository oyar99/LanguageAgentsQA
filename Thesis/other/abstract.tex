\phantomsection
\addcontentsline{toc}{chapter}{Abstract}
\chapter*{Abstract\markboth{Abstract}{}} 

Question Answering (QA) systems receive a natural language query and are required to provide an answer based on a collection of documents. Recently, Large Language Models (LLMs) have demonstrated strong performance on these tasks. However, certain QA problems remain challenging. Multi-Hop Question Answering (MHQA) tasks, which require gathering evidences from multiple sources, and combining information to produce a correct answer, continue to pose difficulties for LLM-based QA systems. In this work, we design and implement three language-agent architectures, DAG Agent, Auto DAG Agent, and Long-lived QA Agent, for solving MHQA tasks based on the \textit{Cognitive Language Agents} framework. We systematically evaluate the performance of these agents using well-known benchmarks, HotpotQA, 2Wiki, and MuSiQue, and compare them against multiple baseline Retrieval-Augmented Generation (RAG) systems that employ lexical, semantic, and graph-based retrievers, as well as several variants of a simpler ReAct agent. The proposed language agents demonstrate significant higher retrieval than the baseline systems, and competitive QA performance, indicating that they effectively leverage decision-making, reasoning, and reflection capabilities to improve multi-hop retrieval. For instance, using GPT-4.1-mini, Auto DAG agent achieves approximately a +5-point improvement in recall on HopotQA and 2WiKi, and a +10-point increase on MuSiQue compared with the baseline agent with GPT-4.1-mini, while exhibiting only a small decline in precision. Similarly, Auto DAG agent achieves competitive performance across the three MHQA benchmarks for Exact Match (EM), ROUGE-1 ($R_1$), and an LLM-based ($L_1$) score. Overall, our results highlight the potential of language-agent architectures for MHQA tasks, demonstrating a clear advantage over traditional RAG systems commonly used in QA. Despite these improvements, language-agent architectures utilize more resources, posing challenges for real-world scalable applications, and show reduced performance on a more diverse dataset, LoCoMo, suggesting limited generalization to other types of questions.