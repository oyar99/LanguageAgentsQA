\cleardoublepage
\chapter{Cognitive Language Agents for Question Answering}
\label{ch:development}
\label{ch:chapter3}

\section{Methodology}
\subsection{Datasets}

To evaluate our language agents and baselines models, we use three  well-known benchmarks for the MHQA task, \textbf{HotpotQA} \cite{yang2018hotpotqa}, \textbf{2WikiMultiHopQA} \cite{ho-etal-2020-constructing}, and \textbf{MuSiQue} \cite{trivedi2021musique}. While \textbf{HotpotQA} has been shown to not require genuine multi-hop reasoning, we include it to support comparison with prior work \cite{trivedi2021musique}.

\noindent All evaluations are conducted on the development sets of these datasets, which offer a sufficient number of samples to yield statistically meaningful results. Following the setup proposed by Bernal et al. \cite{NEURIPS2024_6ddc001d}, we construct a corpus for each dataset by combining the annotated supporting passages with distractor passages. Only answerable questions are retained for our evaluation.

\noindent In addition to the MHQA benchmarks, we evaluate our systems on a lesser-studied dataset \textbf{LoCoMo} designed for conversational QA \cite{maharana-etal-2024-evaluating}. For this work, we select a smaller subset consisting of 10 conversations. The questions include a mix of multi-hop, temporal, open-domain, and single-hop queries. As with the other datasets, non-answerable questions are excluded. Including LoCoMo allows us to test whether the proposed cognitive agent architectures can remain effective in simpler conversational QA scenarios.

\noindent Table \ref{tab:dataset_stats} summarizes key statistics of these datasets. MuSiQue and 2WikiMultiHopQA are the most challenging among the selected datasets. Both were explicitly designed to enforce reasoning and mitigate shortcut-taking by LLMs, making them ideal for evaluating reasoning capabilities of our language agents.

\input{chapter3/datasets_stats}

\subsection{Baselines}
\label{baselines_sec}

We evaluated a range of retrieval systems, including both traditional lexical and semantic retrievers, as well as more advanced methods.

\noindent First, we used a BM25 ranking function with hyperparameters $b = 0.75$ and $k_1 = 0.5$. These values were selected experimentally, considering that passages across all four datasets are relatively short \cite{10.1145/2682862.2682863}. Both  documents and queries were processed through a standard normalization pipeline that includes unigram and bigram generation, stopword removal, Snowball stemming, and other text normalization techniques.

\noindent Second, we evaluated a semantic retriever based on the msmarco-bert-base-dot-v5 model, which encodes text into a 768-dimensional embedding space. This model, trained on question answer pairs from the MS Marco dataset, has demonstrated strong performance on a range of natural language tasks including QA \cite{reimers-2019-sentence-bert}.

\noindent Third, we included ColBERTV2, a semantic retriever that leverages contextual late interaction. Unlike single-vector dense retrievers, ColBERTV2 compares queries and documents at the token level, allowing for finer-grained relevance retrieval \cite{santhanam-etal-2022-colbertv2}.

\noindent To explore retrieval methods that leverage structured knowledge, we evaluated HippoRAG, a recent architecture inspired by neurobiological systems. HippoRAG constructs a knowledge graph by extracting triples from the corpus using an LLM. In our experiments, we used QWen2.5-14B-Instruct \cite{qwen2}, though the architecture is model-agnostic. During inference, HippoRAG employs a semantic retriever to locate relevant nodes in the graph \cite{NEURIPS2024_6ddc001d}, in our case Contriever \cite{lei-etal-2023-unsupervised}. This structured representation enables competitive performance, particularly on more challenging datasets such as MuSiQue.

\noindent To construct a strong baseline for these retrievers, we implemented several standard RAG systems, with carefully designed QA prompt templates to maximize question answering scores. (See Appendix \ref{ch:appendices}). For each system, we varied the number of retrieved passages $k$, from $5$ to $100$, to assess the impact of effective context length on performance. We also tested a full-context variant in which all documents are provided to the model, ensuring that relevant ground-truth passages are always included, even if truncation is needed to fit within the model's context window. Due to computational constraints, questions in the full-context setting are processed in batches of size 8, and the model is required to return structured outputs to locate the answer to each question deterministically. While this multitask scenario is not directly comparable with the RAG baselines, it offers useful insight into how model performance scales as more context is used.

\subsection{Cognitive Language Agents}

We explore several language agent architectures designed to improve QA performance by introducing retrieval and decision-making capabilities at different stages of the pipeline.

\noindent All ouf our agents build on the \textbf{ReAct} framework, which interleaves reasoning and action, where an action is typically a search function. They also decompose the input question into sub-questions to break down complex reasoning steps.

\noindent First, we implement a simple agent that defines a single action to search relevant passages using ColBERTV2. Once the agent decides sufficient context has been gathered, it returns an answer.

\noindent Second, we introduce a re-ranking agent that uses ColBERTV2 for executing searches. Retrieved passages are then pruned with a lightweight LLM-based filter, which prioritizes only the most relevant evidence before reasoning proceeds. This pruning step enables the model to discard distractors and focus its reasoning on higher-quality candidates, effectively performing a re-ranking of the retrieval set. We experiment with several prompt-engineering strategies to guide pruning and assess their impact on downstream accuracy.

\noindent Third, we propose a hybrid retrieval agent that  selects among multiple retrieval strategies, lexical, semantic, or graph-based, depending on the characteristics of the input query. As opposed to systems such AriGraph and HippoRAG, which rely on a single search mechanism \cite{anokhin2024arigraphlearningknowledgegraph}\cite{NEURIPS2024_6ddc001d}, our agent flexibly integrates different retrieval strategies. This allows it to adapt its strategy to the query and improve coverage across diverse information needs.

\noindent Fourth, we design a multi-agent architecture where one agent, responsible for answering questions, interleaves reasoning, action, and feedback from another agent that verifies whether the reasoning process is correct. This interleaved reflection allows the reasoning agent to adjust its trajectory in real time based on feedback, rather that only after reaching a final answer, as is typically done in some QA systems. We experiment with different models for each agent to obtain varying results.

\noindent Finally, these agent are compared against baseline approaches, providing an analysis of their effectiveness, including discussion on system complexity, performance trade-offs, and resource efficiency.


\subsection{Experimental Setup}

Our experiments used GPT-4o-mini (2024-07-18) with a context window of $128k$ tokens. We accessed the model via the OpenAI API using version 2024-12-01-preview, utilizing both the chat completions and batch processing endpoints for cost efficiency.

\noindent We also report results obtained with QWen2.5-14B-Instruct, which supports a $32K$ token context window \cite{qwen2}. This model was executed using vLLM with 16-bit floating precision on two NVIDIA A40 GPUs, each with 46GB of VRAM \cite{kwon2023efficient}.

\noindent For a few scenarios, we additionally evaluated o3-mini (2025-01-31), which provides a $200k$ token context window. For this model, we selected the medium reasoning effort configuration.

\noindent We also experimented with several open-source models, including Qwen2.5-1.5B-Instruct, Gemma 3-27B, Mistral-Nemo-Instruct-2407. However, these models underperformed on more demanding datasets such as 2Wiki and MuSiQue, and thus their results are not emphasized in our analysis.

\noindent All models were run with temperature set to $0$ to ensure deterministic outputs, and frequency and presence penalties were also set to $0$. The maximum number of completion tokens was set to $500$, which we found sufficient, as most answers are brief. Additionally, we designated the newline character (\textbackslash n) as a stop token, since many models, particularly smaller ones, struggled to adhere to the output formatting instructions and often appended additional commentary.