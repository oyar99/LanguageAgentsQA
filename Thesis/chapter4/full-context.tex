\begin{table}[!h]
    \centering
        \setlength{\tabcolsep}{3pt}
        \resizebox{\textwidth}{!}{
        \begin{tabular}{cccccccccccccccccccc}
            \toprule
            \multicolumn{4}{c}{\textbf{LoCoMo}} & \multicolumn{4}{c}{\textbf{HotpotQA}} & \multicolumn{4}{c}{\textbf{2Wiki}} & \multicolumn{4}{c}{\textbf{MuSiQue}} \\
            \cmidrule(lr){1-4} \cmidrule(lr){5-8} \cmidrule(lr){9-12} \cmidrule(lr){13-16}
             EM & $R_1$ & $R_2$ & $L_1$ & EM & $R_1$ & $R_2$ & $L_1$ & EM & $R_1$ & $R_2$ & $L_1$ & EM & $R_1$ & $R_2$ & $L_1$ \\
            \midrule
            16.7 & 40.6 & 21.8 & 43.0 & 42.0 & 54.1 & 29.7 & 54.3 & 30.0 & 35.0 & 22.5 & 33.0 & 19.5 & 29.7 & 15.2 & 23.0 \\
            \bottomrule
        \end{tabular}
        }
    \caption{QA performance of the FULL-CONTEXT setup with GPT-4o-mini ($128k$ tokens), measured using Exact Match (EM), ROUGE-1 ($R_1$), ROUGE-2 ($R_2$), and an LLM-based score ($L_1$).}
    \label{tab:qa_results_fullcontext}
\end{table}